{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "830a2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef667cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta creada : ./reconocimiento_imagenes/DATA/validacion/sentir\n"
     ]
    }
   ],
   "source": [
    "nombre='sentir'  # Ir cambiando para crear las diferentes carpetas\n",
    "direccion= './reconocimiento_imagenes/DATA/validacion'\n",
    "carpeta= direccion + '/'+ nombre\n",
    "if not os.path.exists(carpeta):\n",
    "    print('Carpeta creada :' , carpeta)\n",
    "    os.makedirs(carpeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd7fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont=0   # contador para el nombre de las fotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd05c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # elegir camara web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f031f61",
   "metadata": {},
   "source": [
    " CREAMOS UN OBJETO QUE ALMACENA LA DETECCION Y SEGUIMIENTO DE LAS MANOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4d9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "clase_manos = mp.solutions.hands\n",
    "manos = clase_manos.Hands()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe442983",
   "metadata": {},
   "source": [
    "  METODO PARA DIBUJAR LAS MANOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a91645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibujo=mp.solutions.drawing_utils  # este metodo dibuja 21 puntos de la mano\n",
    "\n",
    "\n",
    "while(1):\n",
    "    ret,frame=cap.read()\n",
    "    color = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    copia = frame.copy()\n",
    "    resultado = manos.process(color)\n",
    "    posiciones = []     # lista donde almacenamos coordenadas de los puntos\n",
    "    #print(resultado.multi_hand_landmarks)    # si queremos ver si existe la deteccion\n",
    "    \n",
    "    if resultado.multi_hand_landmarks:\n",
    "        for mano in resultado.multi_hand_landmarks:\n",
    "            for id,lm in enumerate(mano.landmark):\n",
    "                #print(id,lm)\n",
    "                alto,ancho,c = frame.shape\n",
    "                corx, cory =int(lm.x*ancho), int(lm.y*alto)\n",
    "                posiciones.append([id ,corx,cory])\n",
    "                \n",
    "                dibujo.draw_landmarks(frame, mano, clase_manos.HAND_CONNECTIONS)#lineas entre puntos de la mano\n",
    "            if len(posiciones)!= 0:\n",
    "                pto_i1= posiciones [4]  # numeros de los puntos de la mano a detectar.\n",
    "                pto_i2= posiciones [20]\n",
    "                pto_i3= posiciones [12]\n",
    "                pto_i4= posiciones [0]\n",
    "                pto_i5= posiciones [9]  #punto central \n",
    "                \n",
    "                x1,y1= (pto_i5[1]-200),(pto_i5[2]-200) # -80,-80 /+80,+80 definir cuadro de deteccion\n",
    "                ancho, alto = (x1+200),(y1+200)        \n",
    "                x2,y2= x1+ancho, y1+alto\n",
    "                dedos_reg= copia[y1:y2, x1:x2] # copia las coordenas de arriba porque el recuadro es de 160 no de 80\n",
    "                \n",
    "                cv2.rectangle(frame, (x1,y1) , (x2,y2) , (0,255,0), 3)  # hace rectangulo\n",
    "                \n",
    "            dedos_reg=cv2.resize(dedos_reg,(200,200),interpolation=cv2.INTER_CUBIC) #redimensionamos las fotos\n",
    "            \n",
    "            cv2.imwrite(carpeta + \"/Dedos_{}.jpg\" .format(cont), dedos_reg)\n",
    "            cont=cont+1\n",
    "                \n",
    "                \n",
    "    #contador hasta 300 fotos   \n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)   \n",
    "    k= cv2.waitKey(1)        \n",
    "    if k==27 or cont >= 300:\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07906a9c",
   "metadata": {},
   "source": [
    "# CLASIFICADOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc5ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense,Activation\n",
    "from tensorflow.python.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43cd6630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9058efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()   # para limpiar todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36223c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entrenamiento= './reconocimiento_imagenes/DATA/entrenamiento' # directorios donde estan las fotos\n",
    "datos_validacion= './reconocimiento_imagenes/DATA/validacion' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e1e62",
   "metadata": {},
   "source": [
    "PARAMETROS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f783aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteraciones = 20  #numero de iteraciones para ajustar nuestro modelo\n",
    "altura, longitud= 200,200  #tama単o imagenes de entrenamiento\n",
    "batch_size = 1   #numero de imagenes que vamos a enviar\n",
    "pasos=300/1      #numero de veces que se va a procesar la informacion en cada iteracion\n",
    "pasos_validacion=300/1  #despues de cada iteracion validamos la anterior\n",
    "\n",
    "filtrosconv1=32  \n",
    "filtrosconv2=64  #   numero de filtros que aplicamos en cada iteracion     \n",
    "filtrosconv3=128  # ponemos tres filtros para que abarque mejor los movimientos de la mano\n",
    "\n",
    "tam_filtro1=(4,4)     # 4x4 ,3x3, 2x2\n",
    "tam_filtro2=(3,3)  \n",
    "tam_filtro3=(2,2)   #     tama単o del filtros (1,2,y 3)\n",
    "  \n",
    "\n",
    "tam_pool=(2,2)    #extrae caracteristicas de laimagen de 2x2 para tomar informacion muy detallada\n",
    "clases= 5         # numero de palabras que vamos a hacer\n",
    "lr= 0.0005        #ajustes de la red neuronal, para acceder a una solucion optima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e8abe",
   "metadata": {},
   "source": [
    "PREPROCESAMIENTO DE LAS IMAGENES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2510697",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesamiento_entre= ImageDataGenerator(\n",
    "    rescale= 1./255, # pasar los pixeles de 0 255 | 0 a 1\n",
    "    shear_range=0.3,  #generar nuestras imagenes inclinadas para un mejor entrenamiento\n",
    "    zoom_range=0.3,  #generar imagenes con zoom para un mejor entrenamiento\n",
    "    horizontal_flip= True) #invierte las imagenes para ajustar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e94b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesamiento_val1=ImageDataGenerator( rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dfea1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#IMAGEN ENTRENO\n",
    "imagen_entreno=preprocesamiento_entre.flow_from_directory(\n",
    "    datos_entrenamiento,  #va a tomar las fotos que ya almacenamos\n",
    "    target_size= ( altura,longitud),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical') #clasificacion categorica = por clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e370666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#IMAGEN VALIDACION ( igual que arriba pero validacion)\n",
    "imagen_validacion=preprocesamiento_entre.flow_from_directory(\n",
    "    datos_entrenamiento,\n",
    "    target_size= ( altura,longitud),\n",
    "    batch_size= batch_size,\n",
    "    class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b243e",
   "metadata": {},
   "source": [
    "# CREAMOS LA RED NEURONAL CONVOLUCIONAL (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn= Sequential()  # red neuronal sequencial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa8a7f",
   "metadata": {},
   "source": [
    "Agregamos filtros con el fin de volver nuestra imagen muy profunda pero\n",
    "peque単a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e289635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#es una convolucion y realizamos config\n",
    "\n",
    "cnn.add(Convolution2D(filtrosconv1,\n",
    "                      tam_filtro1, padding='same', input_shape=(altura,longitud,3), activation='relu'))\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=tam_pool)) # despues de la primera capa vamos a tener una capa de max pooling y asignamos el tama単o\n",
    "                                          # maxpooling es la extraccion de caracteristicas\n",
    "\n",
    "    \n",
    "cnn.add(Convolution2D(filtrosconv2,\n",
    "                      tam_filtro2, padding='same', activation='relu')) # agregamos nueva capa\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=tam_pool))\n",
    "\n",
    "#nueva capa\n",
    "\n",
    "cnn.add(Convolution2D(filtrosconv3,\n",
    "                      tam_filtro3, padding='same', activation='relu')) # agregamos nueva capa\n",
    "cnn.add(MaxPooling2D(pool_size=tam_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5922e4",
   "metadata": {},
   "source": [
    "Convertir imagen profunda en imagen plana, para tener una dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "141094c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())  # aplanamos la imagen\n",
    "cnn.add(Dense(640,activation='relu'))  # asignamos 640 neuronas ( 256 ,2 clases, 640,5 clases para calcular con regla de 3)\n",
    "cnn.add(Dropout(0.5)) # apagamos el 50% de las neuronas para ajustar la red( solo entranamiento e validacion utilizar todas)\n",
    "cnn.add(Dense(clases,activation='softmax'))   # dice la probabilidad de que sea de una clase o de otra \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc520a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4db2c4",
   "metadata": {},
   "source": [
    "Agregamos parametros para optimizar el modelo\n",
    "\n",
    "Durante el entrenamiento, tiene autoevaluacion, se optimiza con Adam y la metrica Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4143a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizar=tensorflow.keras.optimizers.Adam(learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0521dcda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x0000013AE4BC9F40>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bceadea96686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m       self.compiled_loss = compile_utils.LossesContainer(\n\u001b[0;32m    572\u001b[0m           loss, loss_weights, output_names=self.output_names)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_single_optimizer\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m       if (loss_scale is not None and\n\u001b[0;32m    601\u001b[0m           not isinstance(opt, lso.LossScaleOptimizer)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    132\u001b[0m         'Could not interpret optimizer identifier: {}'.format(identifier))\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x0000013AE4BC9F40>"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=optimizar, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c2a5b",
   "metadata": {},
   "source": [
    "Entrenamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9644d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(Imagen_entreno , steps_per_epoch = pasos, epochs = iteraciones , validation_date=imagen_validacion , validacion_steps=pasos_validacion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068ea5e",
   "metadata": {},
   "source": [
    "Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('modelopalabras.h5')\n",
    "cnn.save_weights('pesospalabras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ae8b2",
   "metadata": {},
   "source": [
    "# Prediccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fccb9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from keras_preprocessing.image import load_img ,img_to_array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ce516",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo= './modelopalabras.h5'\n",
    "peso='./pesospalabras.h5'\n",
    "cnn=load_model(modelo)   # cargar modelo\n",
    "cnn.load_weights(peso)   #cargar peso\n",
    "\n",
    "direccion='./reconocimiento_imagenes/DATA/validacion'\n",
    "dire_ing = os.listdir(direccion)\n",
    "print('Nombres:  ', dire_ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)   # leemos camara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdd2f3",
   "metadata": {},
   "source": [
    "Creamos un objeto que va a almacenar la deteccion y el seguimiento de las manos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc64b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clase_manos=mp.solutions.hands\n",
    "manos=clase_manos.Hands()  # especificaciones es word\n",
    "\n",
    "dibujo=mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "while(1):\n",
    "    ret,frame=cap.read()\n",
    "    color = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    copia = frame.copy()\n",
    "    resultado = manos.process(color)\n",
    "    posiciones = []     # lista donde almacenamos coordenadas de los puntos\n",
    "    #print(resultado.multi_hand_landmarks)    # si queremos ver si existe la deteccion\n",
    "    \n",
    "    if resultado.multi_hand_landmarks:\n",
    "        for mano in resultado.multi_hand_landmarks:\n",
    "            for id,lm in enumerate(mano.landmark):\n",
    "                #print(id,lm)\n",
    "                alto,ancho,c = frame.shape\n",
    "                corx, cory =int(lm.x*ancho), int(lm.y*alto)\n",
    "                posiciones.append([id ,corx,cory])\n",
    "                \n",
    "                dibujo.draw_landmarks(frame, mano, clase_manos.HAND_CONNECTIONS)#lineas entre puntos de la mano\n",
    "            if len(posiciones)!= 0:\n",
    "                pto_i1= posiciones [4]  # numeros de los puntos de la mano a detectar.\n",
    "                pto_i2= posiciones [20]\n",
    "                pto_i3= posiciones [12]\n",
    "                pto_i4= posiciones [0]\n",
    "                pto_i5= posiciones [9]  #punto central \n",
    "                \n",
    "                x1,y1= (pto_i5[1]-200),(pto_i5[2]-200) # -80,-80 /+80,+80 definir cuadro de deteccion\n",
    "                ancho, alto = (x1+200),(y1+200)        \n",
    "                x2,y2= x1+ancho, y1+alto\n",
    "                dedos_reg= copia[y1:y2, x1:x2] # copia las coordenas de arriba porque el recuadro es de 160 no de 80\n",
    "                dedos_reg=cv2.resize(dedos_reg,(200,200),interpolation=cv2.INTER_CUBIC) #redimensionamos las fotos\n",
    "                x= img_to_array(dedos_reg)\n",
    "                x=np.expand.dims(x, axis=0)\n",
    "                vector=cnn.predict(x)\n",
    "                resultado=vector[0] = [1,0] | [0,1]\n",
    "                respuesta=np.argmax(resultado)\n",
    "                \n",
    "                #vamos poniendo condiciones dependienedo del numero de carpetas de imagenes que tenemos\n",
    "                if respuesta==0:\n",
    "                    print(resultado)\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2) , (0,255,0),3)\n",
    "                    cv2.putText(frame, '{}'.format(dire_img[0]),(x1 , y1-5), 1, 1.3, (0, 255, 0), 1 ,cv2.LINE_AA )\n",
    "                elif respuesta==1:\n",
    "                    print(resultado)\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2) , (0,0,255),3)\n",
    "                    cv2.putText(frame, '{}'.format(dire_img[1]),(x1 , y1-5), 1, 1.3, (0, 255, 0), 1 ,cv2.LINE_AA )\n",
    "                elif respuesta==2:\n",
    "                    print(resultado)\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2) , (255,0,0),3)\n",
    "                    cv2.putText(frame, '{}'.format (dire_img[2]),(x1 , y1-5), 1, 1.3, (0, 255, 0), 1 ,cv2.LINE_AA )\n",
    "                elif respuesta==3:\n",
    "                    print(resultado)\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2) , (255,0,255),3)\n",
    "                    cv2.putText(frame, '{}'.format(dire_img[3]),(x1 , y1-5), 1, 1.3, (0, 255, 0), 1 ,cv2.LINE_AA )\n",
    "                elif respuesta==4:\n",
    "                    print(resultado)\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2) , (0,255,255),3)\n",
    "                    cv2.putText(frame, '{}'.format(dire_img[4]),(x1 , y1-5), 1, 1.3, (0, 255, 0), 1 ,cv2.LINE_AA )\n",
    "                else:\n",
    "                    cv2.putText(frame, ' PALABRA DESCONOCIDA ', (x1 , y1 - 5),1, 1.3, (0, 255, 255), 1 ,cv2.LINE_AA )\n",
    "                    \n",
    "    cv2.imshow('Video',frame)\n",
    "    k= cv2.waikey(1)\n",
    "    if k ==27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
